{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating names with an RNN\n",
    "\n",
    "Edited from [tadeaspaule](https://github.com/tadeaspaule/universal-name-generator/blob/master/Universal%20RNN%20for%20name%20generation.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, concatenate, Reshape, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "# of mascots: 5841\n# of cities: 314\n"
    }
   ],
   "source": [
    "with open('web-scraper/mascots.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    mascots = list(reader)[0]\n",
    "print('# of mascots:', len(mascots))\n",
    "\n",
    "with open('web-scraper/cities.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    cities = list(reader)[0]\n",
    "print('# of cities:', len(cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Total names: 314\nAmount of names after removing those with unwanted characters: 314\nUsing the following characters: [' ', '.', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '–']\nLongest name is 16 characters long\nShortest name is 4 characters long\n"
    }
   ],
   "source": [
    "def process_names(names, *, unwanted=['(', ')', '-', '.', '/', '\\xa0', '&', '!']):\n",
    "    # names = [name.lower() for name in names]\n",
    "    print(\"Total names:\", len(names))\n",
    "    chars = sorted(list(set(''.join(names))))\n",
    "\n",
    "    def has_unwanted(word):\n",
    "        for char in word:\n",
    "            if char in unwanted:\n",
    "                return True\n",
    "        return False\n",
    "    names = [name for name in names if not has_unwanted(name)]\n",
    "    print(\"Amount of names after removing those with unwanted characters:\", len(names))\n",
    "    chars = [char for char in chars if char not in unwanted]\n",
    "    print(\"Using the following characters:\", chars)\n",
    "\n",
    "    maxlen = max([len(name) for name in names])\n",
    "    minlen = min([len(name) for name in names])\n",
    "    print(\"Longest name is\", maxlen, \"characters long\")\n",
    "    print(\"Shortest name is\", minlen, \"characters long\")\n",
    "    \n",
    "    # enchar indicates the end of the word\n",
    "    # here it goes through unlikely-to-be-used characters to find one it can use\n",
    "    endchars = '!£$%^&*()-_=+/?.>,<;:@[{}]#~'\n",
    "    endchar = [ch for ch in endchars if ch not in chars][0]\n",
    "\n",
    "    # ensures the character isn't already used & present in the training data\n",
    "    assert(endchar not in chars)\n",
    "    chars += endchar\n",
    "    \n",
    "    return names, chars\n",
    "\n",
    "# names = cc.get_city_list(['Germany'])\n",
    "names, chars = process_names(cities, unwanted=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Getting the X - long sequences\n",
    "- This model basically works by looking at X characters (in this case 4), and predicting what the next character will be\n",
    "- Changing this X value will affect what patterns the model learns, if we make X too big it can simply memorize names from the dataset, but if we make it too small, it won't be able to accurately predict the next character\n",
    "- I played around with it a bit and settled on 4, but feel free to try out different values (you should only have to change the value below in seqlen = 4 and the rest of the code will adjust itself based on that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "34948 sequences of length 4 made\n"
    }
   ],
   "source": [
    "def make_sequences(names, seqlen):\n",
    "    # To have the model learn a more macro understanding, it also takes the word's length so far as input\n",
    "    sequences, lengths, nextchars = [], [], []\n",
    "\n",
    "    for name in names:\n",
    "        if len(name) <= seqlen:\n",
    "            sequences.append(name + chars[-1]*(seqlen - len(name)))\n",
    "            nextchars.append(chars[-1])\n",
    "            lengths.append(len(name))\n",
    "        else:\n",
    "            for i in range(0, len(name) - seqlen + 1):\n",
    "                sequences.append(name[i:i + seqlen])\n",
    "                if i+seqlen < len(name):\n",
    "                    nextchars.append(name[i + seqlen])\n",
    "                else:\n",
    "                    nextchars.append(chars[-1])\n",
    "                lengths.append(i + seqlen)\n",
    "\n",
    "    print(len(sequences), \"sequences of length\", seqlen, \"made\")\n",
    "    return sequences,lengths,nextchars\n",
    "\n",
    "seqlen = 4\n",
    "sequences, lengths, nextchars = make_sequences(names, seqlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. One hot encoding the sequences, word lengths, and next characters\n",
    "- One hot encoding means that, for example, if you have 5 characters that can appear, you turn the first character into [1 0 0 0 0], the second into [0 1 0 0 0], and so on\n",
    "- We do it because this format is easy for the model to read (and we need to somehow turn the sequence strings into number values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_onehots(*, sequences, lengths, nextchars, chars):\n",
    "    x = np.zeros(shape=(len(sequences),len(sequences[0]), len(chars)), dtype='float32') # sequences\n",
    "    x2 = np.zeros(shape=(len(lengths), max(lengths))) # lengths\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, char in enumerate(seq):\n",
    "            x[i, j, chars.index(char)] = 1.\n",
    "\n",
    "    for i, l in enumerate(lengths):\n",
    "        x2[i, l-1] = 1.\n",
    "\n",
    "    y = np.zeros(shape=(len(nextchars),len(chars)))\n",
    "    for i, char in enumerate(nextchars):\n",
    "        y[i, chars.index(char)] = 1.\n",
    "    \n",
    "    return x, x2, y\n",
    "\n",
    "x, x2, y = make_onehots(sequences=sequences,\n",
    "                        lengths=lengths,\n",
    "                        nextchars=nextchars,\n",
    "                        chars=chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Method for generating random starting sequences\n",
    "- Looks at the probabilities letters appear after each other (for example, how often is 'a' third when 'f' is second, compared to other letters that occur after a second 'f')\n",
    "- We will use this later to make brand new names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictchars(names, seqlen):\n",
    "    dictchars = [{} for _ in range(seqlen)]\n",
    "\n",
    "    for name in names:\n",
    "        if len(name) < seqlen:\n",
    "            continue\n",
    "        dictchars[0][name[0]] = dictchars[0].get(name[0], 0) + 1\n",
    "        for i in range(1, seqlen):\n",
    "            if dictchars[i].get(name[i-1], 0) == 0:\n",
    "                dictchars[i][name[i-1]] = {name[i]: 1}\n",
    "            elif dictchars[i][name[i-1]].get(name[i], 0) == 0:\n",
    "                dictchars[i][name[i-1]][name[i]] = 1\n",
    "            else:\n",
    "                dictchars[i][name[i-1]][name[i]] += 1\n",
    "\n",
    "    return dictchars\n",
    "                \n",
    "dictchars = get_dictchars(names, seqlen)\n",
    "                \n",
    "'''\n",
    "What is dictchars?\n",
    "Basically, stores how often a letter occurs after another letter at a specific spot in a name\n",
    "\n",
    "dictchars[0] just stores how often each letter is first, {a: 3, b:4, etc}\n",
    "\n",
    "dictchars[1+] store which letters (and how often) come after a certain letter.\n",
    "For example, if dictchars[1]['a'] = {b:4,c:1}, that means that if 'a' was first, \n",
    "b followed 4 times, while c followed only once.\n",
    "\n",
    "This is used in the method below to generate plausible-sounding starting sequences.\n",
    "'''\n",
    "    \n",
    "\n",
    "def generate_start_seq(dictchars):\n",
    "    res = \"\" # The starting sequence will be stored here\n",
    "    p = sum([n for n in dictchars[0].values()]) # total amount of letter occurences\n",
    "    r = np.random.randint(0, p) # random number used to pick the next character\n",
    "    tot = 0\n",
    "    for key, item in dictchars[0].items():\n",
    "        if r >= tot and r < tot + item:\n",
    "            res += key\n",
    "            break\n",
    "        else:\n",
    "            tot += item\n",
    "\n",
    "    for i in range(1, len(dictchars)):\n",
    "        ch = res[-1]\n",
    "        if dictchars[i].get(ch, 0) == 0:\n",
    "            l = list(dictchars[i].keys())\n",
    "            ch = l[np.random.randint(0, len(l))]\n",
    "        p = sum([n for n in dictchars[i][ch].values()])\n",
    "        r = np.random.randint(0, p)\n",
    "        tot = 0\n",
    "        for key, item in dictchars[i][ch].items():\n",
    "            if r >= tot and r < tot + item:\n",
    "                res += key\n",
    "                break\n",
    "            else:\n",
    "                tot += item\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Methods for generating text\n",
    "- The methods below basically take care of 'I give X letters, I get the full name', so that we can easily monitor the progress of the model (and combined with the above-declared method that makes random starting sequences, we won't even need to provide anything to get brand new names)\n",
    "- There is one concept used below called <i>temperature</i>. Basically it's a measure randomness plays when selecting the next letter, with 0 being no randomness, always picking the most likely letter, and 1 being total randomness, and the letters are chosen based on their probability value\n",
    "- Adjusting this changes how your generated names look, typically the closer to 0 you are the more coherent and closely resembling the training data the output is, and the closer to 1 you are the more novel but sometimes also less coherent the output is. This mainly affects large generated texts though, not so much names. Nevertheless, I tend to go for ~0.4 temperature usually, but feel free to try out different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=0.4):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    if temperature == 0:\n",
    "        # Avoiding a division by 0 error\n",
    "        return np.argmax(preds)\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_name(model, start, *, chars=chars, temperature=0.4):\n",
    "    maxlength = model.layers[3].input.shape[1]\n",
    "    seqlen = int(model.layers[0].input.shape[1])\n",
    "    result = start\n",
    "    \n",
    "    sequence_input = np.zeros(shape=(1, seqlen, len(chars)))\n",
    "    for i, char in enumerate(start):\n",
    "        sequence_input[0, i, chars.index(char)] = 1.\n",
    "    \n",
    "    length_input = np.zeros(shape=(1, maxlength))\n",
    "    length_input[0, len(result)-1] = 1.\n",
    "    \n",
    "    prediction = model.predict(x=[sequence_input, length_input])[0]\n",
    "    char_index = sample(prediction, temperature)\n",
    "    while char_index < len(chars)-1 and len(result) < maxlength:\n",
    "        result += chars[char_index]\n",
    "        \n",
    "        sequence_input = np.zeros(shape=(1, seqlen, len(chars)))\n",
    "        for i, char in enumerate(result[(-seqlen):]):\n",
    "            sequence_input[0, i,chars.index(char)] = 1.\n",
    "        \n",
    "        length_input[0, len(result)-2] = 0.\n",
    "        length_input[0, len(result)-1] = 1.\n",
    "        \n",
    "        prediction = model.predict(x=[sequence_input, length_input])[0]\n",
    "        char_index = sample(prediction, temperature)\n",
    "    \n",
    "    return result.title()\n",
    "\n",
    "def generate_random_name(model, *, chars=chars, dictchars=dictchars, temperature=0.4):\n",
    "    start = generate_start_seq(dictchars)\n",
    "    return generate_name(model, start, chars=chars, temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Building the model\n",
    "- Here is where you can experiment and try out different approaches\n",
    "- After some testing, I went with the below setup:\n",
    "    - 2 Inputs (the sequence, and the one-hot-encoded length of the name at the end of that sequence)\n",
    "    - 2 parallel LSTM layers, one normal with relu, the other backwards with tanh, both with dropout 0.3\n",
    "    - Concatenate the LSTM outputs with the one-hot-encoded length\n",
    "    - Dense output layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(x, x2, chars):\n",
    "    inp1 = Input(shape=x.shape[1:]) # sequence input\n",
    "    inp2 = Input(shape=x2.shape[1:]) # length input\n",
    "    lstm = LSTM(len(chars), activation='relu', dropout=0.3)(inp1)\n",
    "    lstm2 = LSTM(len(chars), dropout=0.3, go_backwards=True)(inp1)\n",
    "    concat = concatenate([lstm, lstm2, inp2])\n",
    "    dense = Dense(len(chars), activation='softmax')(concat)\n",
    "\n",
    "    model = Model([inp1, inp2], dense)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "model = make_model(x, x2, chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Method for training a model and monitoring its progress\n",
    "- Using this makes it easy to try out different model architectures and see what names they are able to generate\n",
    "- For fast prototyping, just build and compile a model, then simply:\n",
    "```python\n",
    "try_model(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model(model, *, x=x, x2=x2, y=y, chars=chars, dictchars=dictchars, total_epochs=180, print_every=60, temperature=0.4 ,verbose=True):\n",
    "    for i in range(total_epochs//print_every):\n",
    "        history = model.fit([x, x2],\n",
    "                            y, \n",
    "                            epochs=print_every,\n",
    "                            batch_size=64,\n",
    "                            validation_split=0.05,\n",
    "                            verbose=0)\n",
    "        if verbose:\n",
    "            print(\"\\nEpoch\", (i+1) * print_every)\n",
    "            print(\"First loss:            %1.4f\" % (history.history['loss'][0]))\n",
    "            print(\"Last loss:             %1.4f\" % (history.history['loss'][-1]))\n",
    "            print(\"First validation loss: %1.4f\" % (history.history['val_loss'][0]))\n",
    "            print(\"Last validation loss:  %1.4f\" % (history.history['val_loss'][-1]))\n",
    "            print(\"\\nGenerating random names:\")\n",
    "            for _ in range(10):\n",
    "                print(generate_random_name(model, chars=chars, dictchars=dictchars, temperature=temperature)) \n",
    "    if not verbose:\n",
    "        print(\"Model training complete, here are some generated names:\")\n",
    "        for _ in range(20):\n",
    "            print(generate_random_name(model, chars=chars, dictchars=dictchars, temperature=0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. And finally, training the model and seeing how it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "# Putting it all together to showcase other datasets\n",
    "### This method returns a model and a method for generating more names\n",
    "```python\n",
    "# usage example\n",
    "names = load_name_data() # load your desired name dataset\n",
    "model, generate_name = train_model(names)\n",
    "\n",
    "new_names = []\n",
    "for _ in range(1000):\n",
    "    new_name = generate_name()\n",
    "    new_names.append(new_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(names, *,  seqlen=4, unwanted=['(',  ')',  '-',  '.',  '/'], verbose=True):\n",
    "    names, chars = process_names(names, unwanted=unwanted)\n",
    "    \n",
    "    sequences, lengths, nextchars = make_sequences(names, seqlen)\n",
    "    \n",
    "    x, x2, y = make_onehots(sequences=sequences,\n",
    "                          lengths=lengths,\n",
    "                          nextchars=nextchars,\n",
    "                          chars=chars)\n",
    "        \n",
    "    dictchars = get_dictchars(names, seqlen)\n",
    "    \n",
    "    model = make_model(x, x2, chars)  \n",
    "    \n",
    "    try_model(model, x=x, x2=x2, y=y, chars=chars, dictchars=dictchars, verbose=verbose)\n",
    "    \n",
    "    def generate():\n",
    "        return generate_random_name(model, chars=chars, dictchars=dictchars, temperature=0.4)\n",
    "    \n",
    "    print('Done training.')\n",
    "    return model, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Total names: 5841\nAmount of names after removing those with unwanted characters: 5677\nUsing the following characters: [' ', \"'\", '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\nLongest name is 27 characters long\nShortest name is 2 characters long\n34948 sequences of length 4 made\nModel training complete, here are some generated names:\nHurbons\nSteffclowbors\nPells\nRolchts\nFubrons\nKag Hornets\nDaves\nLighting Blues\nCaskers\nZighinas\nChewks\nHaspbuts\nGrain Bears\nZiver Pirates\nSpants\nNoyralds\nSorballers\nPaeers\nGrints\nGroomers\nDone training.\n"
    },
    {
     "ename": "TypeError",
     "evalue": "generate_name() missing 2 required positional arguments: 'model' and 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-c8a1a2f363c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_mascots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnew_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnew_mascots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mascots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_name() missing 2 required positional arguments: 'model' and 'start'"
     ]
    }
   ],
   "source": [
    "# Train mascot model\n",
    "mascot_model, mascot_gen = train_model(mascots, unwanted=['(', ')', '-', '.', '/', '\\xa0', '&', '!'], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Total names: 314\nAmount of names after removing those with unwanted characters: 314\nUsing the following characters: [' ', '.', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '–']\nLongest name is 16 characters long\nShortest name is 4 characters long\n1811 sequences of length 4 made\nModel training complete, here are some generated names:\nOxn Valley\nSprrngfield\nBurinas\nMantion\nMiley\nGaland Rapids\nMexington\nShester\nNolida\nEllton\nRorwalk\nScernsville\nAtlentown\nPoille\nHompton\nDorwalk\nDalarmon\nNen Orlen\nNorrane\nFacoland\nDone training.\n"
    }
   ],
   "source": [
    "# Train mascot model\n",
    "city_model, city_gen = train_model(cities, unwanted=[], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mascot_model.save('mascot_model.h5')\n",
    "mascot_model.summary()\n",
    "# new_model = tf.keras.models.load_model('mascot_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_4\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_9 (InputLayer)            [(None, 4, 52)]      0                                            \n__________________________________________________________________________________________________\nlstm_8 (LSTM)                   (None, 52)           21840       input_9[0][0]                    \n__________________________________________________________________________________________________\nlstm_9 (LSTM)                   (None, 52)           21840       input_9[0][0]                    \n__________________________________________________________________________________________________\ninput_10 (InputLayer)           [(None, 16)]         0                                            \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 120)          0           lstm_8[0][0]                     \n                                                                 lstm_9[0][0]                     \n                                                                 input_10[0][0]                   \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 52)           6292        concatenate_4[0][0]              \n==================================================================================================\nTotal params: 49,972\nTrainable params: 49,972\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "city_model.save('city_model.h5')\n",
    "city_model.summary()\n",
    "# new_model = tf.keras.models.load_model('city_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Reaners\nPoorest Lakes\nRighlanders\nCooses\nBrockers\nBichards\nRars\nRostling Train\nLate Vikes\nFrest\nGuld Riders\nGenicans\nFiolden Warriors\nAngulas\nJalos\nRowks\nDritters\nOridgets\nGlken Devils\nArimen\n"
    }
   ],
   "source": [
    "new_mascots = []\n",
    "for _ in range(20):\n",
    "    new_mascots.append(mascot_gen())\n",
    "print(*new_mascots, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Kiten\nKano\nCouna Valle\nPiringfield\nLemerton\nFalens\nVirkson\nLavelley\nDaison\nVincester\nCanta Rouge\nStarkan\nBartin\nDanco\nNoshas\nLelburgh\nEllley\nHarwalk\nGrlleton\nWoleyword\n"
    }
   ],
   "source": [
    "new_cities = []\n",
    "for _ in range(20):\n",
    "    new_cities.append(city_gen())\n",
    "print(*new_cities, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}